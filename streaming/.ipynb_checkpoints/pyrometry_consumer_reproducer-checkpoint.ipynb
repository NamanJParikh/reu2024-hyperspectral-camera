{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e73878a-cab1-495f-acc4-9023a8e58b64",
   "metadata": {},
   "source": [
    "# Consumer for Performing Pyrometry Analysis\n",
    "This consumes the hyprespectral camera data streamed from the initial producer and analyzes it to determine temperatures and emissivities. The code is structured off of notebooks at github.com/openmsi/openmsistream_short_course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdbd5b0-ae6e-4762-a610-35242f932d89",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "260d7094-39ba-40b1-aa5c-261d45c9fb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pathlib, importlib, logging, datetime, json, platform\n",
    "from threading import Thread\n",
    "from openmsitoolbox.logging import OpenMSILogger\n",
    "from openmsistream import (\n",
    "    DataFileDownloadDirectory,\n",
    "    DataFileStreamProcessor,\n",
    "    MetadataJSONReproducer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df3aac10-b316-4793-b6c8-7de3d222a30e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'logging' from '/Users/namanparikh/opt/anaconda3/envs/openmsi/lib/python3.9/logging/__init__.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure a logger (only needed when running in a Jupyter notebook like this)\n",
    "logger = OpenMSILogger(\"OpenMSIConsumers\", filelevel=None)\n",
    "importlib.reload(logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "700f7396-9010-4a4d-90d2-3c84a6d23625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the topic to consume files from\n",
    "CONSUMER_TOPIC_NAME = \"tutorial_data\"\n",
    "\n",
    "# Path to the root directory of this repo\n",
    "repo_root_dir = pathlib.Path().resolve().parent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5cfdf0-2e0d-48f3-a72b-33e531475ae5",
   "metadata": {},
   "source": [
    "## Consuming to the Local Filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "818142fa-e9a6-42fb-a214-b38cb76d2dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_task(download_directory):\n",
    "    \"\"\"Run \"reconstruct\" for a given DataFileDownloadDirectory, and log some messages\n",
    "    when it gets shut down\n",
    "\n",
    "    Args:\n",
    "        download_directory (DataFileDownloadDirectory): the DataFileDownloadDirectory to run\n",
    "    \"\"\"\n",
    "    # This call to \"reconstruct\" waits until the program is shut down\n",
    "    (\n",
    "        n_read,\n",
    "        n_processed,\n",
    "        n_complete_files,\n",
    "        complete_filepaths,\n",
    "    ) = download_directory.reconstruct()\n",
    "    download_directory.close()\n",
    "    msg = f\"{n_read} total messages were consumed\"\n",
    "    if len(complete_filepaths) > 0:\n",
    "        msg += (\n",
    "            f\", {n_processed} messages were successfully processed, and \"\n",
    "            f'{n_complete_files} file{\" was\" if n_complete_files==1 else \"s were\"} '\n",
    "            \"successfully reconstructed\"\n",
    "        )\n",
    "    else:\n",
    "        msg += f\" and {n_processed} messages were successfully processed\"\n",
    "    msg += (\n",
    "        f\". Most recent completed files (up to {download_directory.N_RECENT_FILES}):\\n\\t\"\n",
    "    )\n",
    "    msg += \"\\n\\t\".join([str(filepath) for filepath in complete_filepaths])\n",
    "    download_directory.logger.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bafef60d-23b2-4866-8b88-528ea06c122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the config file and the directory holding the test files\n",
    "CONFIG_FILE_PATH = repo_root_dir / \"streaming\" / \"config_files\" / \"confluent_cloud_broker.config\"\n",
    "TEST_RECO_DIR = repo_root_dir / \"streaming\" / \"reconstructed_test_folder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "760709e2-32b0-484a-84cd-5be2b6055298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/namanparikh/Documents/GitHub/paradim/reu2024-hyperspectral-camera/streaming/config_files/confluent_cloud_broker.config')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4823792f-6dfb-450f-8647-3430c7f36a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/namanparikh/Documents/GitHub/paradim/reu2024-hyperspectral-camera/streaming/reconstructed_test_folder')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_RECO_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef4cd7e6-216f-400d-8e8d-51ea373f66cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[OpenMSIConsumers 2024-07-18 09:38:42] Will reconstruct files from messages in the tutorial_data topic using 2 threads\n",
      "[OpenMSIConsumers 2024-07-18 09:38:45] 2 total messages were consumed, 2 messages were successfully processed, and 2 files were successfully reconstructed. Most recent completed files (up to 50):\n",
      "\ttest_data.txt\n",
      "\t.ipynb_checkpoints/test_data-checkpoint.txt\n"
     ]
    }
   ],
   "source": [
    "# Create the DataFileDownloadDirectory\n",
    "dfdd = DataFileDownloadDirectory(\n",
    "    TEST_RECO_DIR,\n",
    "    CONFIG_FILE_PATH,\n",
    "    CONSUMER_TOPIC_NAME,\n",
    "    logger=logger,\n",
    ")\n",
    "# Start running its \"reconstruct\" function in a separate thread\n",
    "download_thread = Thread(\n",
    "    target=download_task,\n",
    "    args=(dfdd,),\n",
    ")\n",
    "download_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b8aba9b-8a3a-436f-809f-f6732c02397d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually shut down the download directory (if running from the command line this would\n",
    "# be like typing \"q\" in the Terminal window)\n",
    "dfdd.control_command_queue.put(\"q\")\n",
    "download_thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ad2dc9-11aa-4821-bcf4-2790f2442c67",
   "metadata": {},
   "source": [
    "## Stream Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "317f7781-554d-45c6-9435-afc26d83cd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlaceholderStreamProcessor(DataFileStreamProcessor):\n",
    "    \"\"\"Performs a placeholder task (writing out a file to the local system) for every\n",
    "    data file reconstructed from a topic\n",
    "    \"\"\"\n",
    "\n",
    "    def _process_downloaded_data_file(self, datafile, lock):\n",
    "        \"Writes out a file with a timestamp for each reconstructed file\"\n",
    "        try:\n",
    "            timestamp = datetime.datetime.now()\n",
    "            rel_filepath = datafile.relative_filepath\n",
    "            rel_fp_str = str(rel_filepath.as_posix()).replace(\"/\",\"_\").replace(\".\",\"_\")\n",
    "            output_filepath = self._output_dir / f\"{rel_fp_str}_placeholder.txt\"\n",
    "            with lock:\n",
    "                with open(output_filepath, \"w\") as filep:\n",
    "                    filep.write(\n",
    "                        f\"Processing timestamp: {timestamp.strftime('%m/%d/%Y, %H:%M:%S')}\"\n",
    "                    )\n",
    "        except Exception as exc:\n",
    "            return exc\n",
    "        return None\n",
    "\n",
    "    @classmethod\n",
    "    def run_from_command_line(cls, args=None):\n",
    "        \"Not used in this example... stay tuned for the live coding tomorrow!\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d839182d-4bde-4908-8d21-a1dd102ee272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_processor_task(stream_processor):\n",
    "    \"\"\"Run \"process_files_as_read\" for the given stream processor, and log a message\n",
    "    when it gets shuts down\n",
    "    \n",
    "    Args:\n",
    "        stream_processor (openmsistream.DataFileStreamProcessor): The stream processor to run\n",
    "    \"\"\"\n",
    "    # This call to \"process_files_as_read\" hangs until the stream processor is shut down\n",
    "    (\n",
    "        n_m_r, # The number of messages read\n",
    "        n_m_p, # The number of messages processed\n",
    "        n_f_p, # The number of files successfully processed\n",
    "        p_fps, # Paths to the most recently-processed files\n",
    "    ) = stream_processor.process_files_as_read()\n",
    "    stream_processor.close()\n",
    "    msg = f\"{n_m_r} total messages were consumed\"\n",
    "    if n_f_p > 0:\n",
    "        msg += (\n",
    "            f\", {n_m_p} messages were processed,\"\n",
    "            f\" and {n_f_p} files were successfully processed\"\n",
    "        )\n",
    "    else:\n",
    "        msg += f\" and {n_m_p} messages were successfully processed\"\n",
    "    msg += (\n",
    "        f\". Up to {stream_processor.N_RECENT_FILES} most recently \"\n",
    "        \"processed files:\\n\\t\"\n",
    "    )\n",
    "    msg += \"\\n\\t\".join([str(fp) for fp in p_fps])\n",
    "    stream_processor.logger.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "701783a7-56f5-4757-9c07-731677f75f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory to store the StreamProcessor output\n",
    "STREAM_PROCESSOR_OUTPUT_DIR = repo_root_dir / \"streaming\" / \"PlaceholderStreamProcessor_output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d30e9cd3-f93c-474f-9c2b-f6486dc36ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[OpenMSIConsumers 2024-07-18 09:36:35] Log files and output will be in /Users/namanparikh/Documents/GitHub/paradim/PlaceholderStreamProcessor_output\n",
      "[OpenMSIConsumers 2024-07-18 09:36:35] Will process files from messages in the tutorial_data topic using 2 threads\n",
      "[OpenMSIConsumers 2024-07-18 09:38:38] 11 total messages were consumed, 11 messages were processed, and 11 files were successfully processed. Up to 50 most recently processed files:\n",
      "\t.ipynb_checkpoints/test_data-checkpoint.txt\n",
      "\ttest_data.txt\n",
      "\ttest_watchdog.txt\n",
      "\ttest_watchdog_folder/test_1.txt\n",
      "\ttest_watchdog_folder/.ipynb_checkpoints/test_1-checkpoint.txt\n",
      "\ttest_watchdog_folder/test_2.txt\n",
      "\ttest_watchdog_folder/.ipynb_checkpoints/test_2-checkpoint.txt\n",
      "\ttest_watchdog_process_folder/test_2.txt\n",
      "\ttest_watchdog_process_folder/.ipynb_checkpoints/test_1-checkpoint.txt\n",
      "\ttest_watchdog_process_folder/test_1.txt\n",
      "\ttest_watchdog_process_folder/.ipynb_checkpoints/test_2-checkpoint.txt\n"
     ]
    }
   ],
   "source": [
    "# Create the StreamProcessor\n",
    "psp = PlaceholderStreamProcessor(\n",
    "    CONFIG_FILE_PATH,\n",
    "    CONSUMER_TOPIC_NAME,\n",
    "    output_dir=STREAM_PROCESSOR_OUTPUT_DIR,\n",
    "    logger=logger,\n",
    ")\n",
    "# Start running its \"process_files_as_read\" function in a separate thread\n",
    "processor_thread = Thread(\n",
    "    target=stream_processor_task,\n",
    "    args=(psp,),\n",
    ")\n",
    "processor_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1daa2254-9378-483b-a20f-a12a0a463256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually shut down the stream processor (if running from the command line this would\n",
    "# be like typing \"q\" in the Terminal window)\n",
    "psp.control_command_queue.put(\"q\")\n",
    "processor_thread.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
