{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d733905d-cdf9-4eb8-aac6-6adf4dd1b7ff",
   "metadata": {},
   "source": [
    "# Processor of Camera Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a36b2c56-25ce-4680-a964-68b28a0ac549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pathlib, importlib, logging, datetime, json, platform\n",
    "from threading import Thread\n",
    "from openmsitoolbox.logging import OpenMSILogger\n",
    "from openmsistream import (\n",
    "    DataFileDownloadDirectory,\n",
    "    DataFileStreamProcessor,\n",
    "    MetadataJSONReproducer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0d7f6f5-0277-4f90-9e1d-0975a63f0f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'logging' from '/Users/namanparikh/opt/anaconda3/envs/openmsi/lib/python3.9/logging/__init__.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure a logger (only needed when running in a Jupyter notebook like this)\n",
    "logger = OpenMSILogger(\"OpenMSIConsumers\", filelevel=None)\n",
    "importlib.reload(logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d66516f4-9137-49c5-b696-106af8fcfa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the topic to consume files from\n",
    "CONSUMER_TOPIC_NAME = \"tutorial_data\"\n",
    "TOPIC_NAME = \"tutorial_metadata\"\n",
    "\n",
    "# Path to the root directory of this repo\n",
    "repo_root_dir = pathlib.Path().resolve().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2137a1d8-0440-4955-bc11-7080c7c9d4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the config file and the directory holding the test files\n",
    "CONFIG_FILE_PATH = repo_root_dir / \"streaming_2\" / \"config_files\" / \"confluent_cloud_broker.config\"\n",
    "TEST_RECO_DIR = repo_root_dir.parent / \"streaming_2\" / \"reconstructed_test_files\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db99b912-11dc-44ce-a6d2-044246072ab8",
   "metadata": {},
   "source": [
    "## Stream Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9cb33e1-900b-4705-b6d4-80755f8ce219",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlaceholderStreamProcessor(DataFileStreamProcessor):\n",
    "    \"\"\"Performs a placeholder task (writing out a file to the local system) for every\n",
    "    data file reconstructed from a topic\n",
    "    \"\"\"\n",
    "\n",
    "    def _process_downloaded_data_file(self, datafile, lock):\n",
    "        \"Writes out a file with a timestamp for each reconstructed file\"\n",
    "        try:\n",
    "            timestamp = datetime.datetime.now()\n",
    "            rel_filepath = datafile.relative_filepath\n",
    "            rel_fp_str = str(rel_filepath.as_posix()).replace(\"/\",\"_\").replace(\".\",\"_\")\n",
    "            output_filepath = self._output_dir / f\"{rel_fp_str}_placeholder.txt\"\n",
    "            with lock:\n",
    "                arr = np.zeros((3,3))\n",
    "                np.save(output_filepath, arr)\n",
    "                upload_file = UploadDataFile(upload_file_path, rootdir=self._output_dir, logger=logger)\n",
    "                upload_file.upload_whole_file(CONFIG_FILE_PATH, TOPIC_NAME)\n",
    "        except Exception as exc:\n",
    "            return exc\n",
    "        return None\n",
    "    \n",
    "    @classmethod\n",
    "    def run_from_command_line(cls, args=None):\n",
    "        \"Not used in this example... stay tuned for the live coding tomorrow!\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcc86729-3302-42dc-94ac-0753ada54e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_processor_task(stream_processor):\n",
    "    \"\"\"Run \"process_files_as_read\" for the given stream processor, and log a message\n",
    "    when it gets shuts down\n",
    "    \n",
    "    Args:\n",
    "        stream_processor (openmsistream.DataFileStreamProcessor): The stream processor to run\n",
    "    \"\"\"\n",
    "    # This call to \"process_files_as_read\" hangs until the stream processor is shut down\n",
    "    (\n",
    "        n_m_r, # The number of messages read\n",
    "        n_m_p, # The number of messages processed\n",
    "        n_f_p, # The number of files successfully processed\n",
    "        p_fps, # Paths to the most recently-processed files\n",
    "    ) = stream_processor.process_files_as_read()\n",
    "    stream_processor.close()\n",
    "    msg = f\"{n_m_r} total messages were consumed\"\n",
    "    if n_f_p > 0:\n",
    "        msg += (\n",
    "            f\", {n_m_p} messages were processed,\"\n",
    "            f\" and {n_f_p} files were successfully processed\"\n",
    "        )\n",
    "    else:\n",
    "        msg += f\" and {n_m_p} messages were successfully processed\"\n",
    "    msg += (\n",
    "        f\". Up to {stream_processor.N_RECENT_FILES} most recently \"\n",
    "        \"processed files:\\n\\t\"\n",
    "    )\n",
    "    msg += \"\\n\\t\".join([str(fp) for fp in p_fps])\n",
    "    stream_processor.logger.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cec14a3c-5bbb-4fd2-a2b0-d00b2311c2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory to store the StreamProcessor output\n",
    "STREAM_PROCESSOR_OUTPUT_DIR = repo_root_dir / \"streaming_2\" / \"processor_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b317995-78cd-4126-9726-f9905a05b1fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/namanparikh/Documents/GitHub/paradim/reu2024-hyperspectral-camera/streaming_2/processor_1')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STREAM_PROCESSOR_OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278b552d-e0f9-4008-87f0-238bcc10938c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the StreamProcessor\n",
    "psp = PlaceholderStreamProcessor(\n",
    "    CONFIG_FILE_PATH,\n",
    "    CONSUMER_TOPIC_NAME,\n",
    "    output_dir=STREAM_PROCESSOR_OUTPUT_DIR,\n",
    "    logger=logger,\n",
    ")\n",
    "# Start running its \"process_files_as_read\" function in a separate thread\n",
    "processor_thread = Thread(\n",
    "    target=stream_processor_task,\n",
    "    args=(psp,),\n",
    ")\n",
    "processor_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3f6d1a-b380-4d3d-a892-4cf59e559be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually shut down the stream processor (if running from the command line this would\n",
    "# be like typing \"q\" in the Terminal window)\n",
    "psp.control_command_queue.put(\"q\")\n",
    "processor_thread.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
