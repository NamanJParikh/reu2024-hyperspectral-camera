{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7016054f-7685-45aa-ab57-3af6647282c8",
   "metadata": {},
   "source": [
    "# Processor of Result Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f509096-4a2f-4138-a703-f88ca953c842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pathlib, importlib, logging, datetime, json, platform\n",
    "from threading import Thread\n",
    "from openmsitoolbox.logging import OpenMSILogger\n",
    "from openmsistream import (\n",
    "    DataFileDownloadDirectory,\n",
    "    DataFileStreamProcessor,\n",
    "    MetadataJSONReproducer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07deb3a3-421d-411b-9ac2-f753b742f290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'logging' from '/Users/namanparikh/opt/anaconda3/envs/openmsi/lib/python3.9/logging/__init__.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure a logger (only needed when running in a Jupyter notebook like this)\n",
    "logger = OpenMSILogger(\"OpenMSIConsumers\", filelevel=None)\n",
    "importlib.reload(logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df6b0809-0b1c-4be8-91b4-7bdd858e9f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the topic to consume files from\n",
    "CONSUMER_TOPIC_NAME = \"tutorial_metadata\"\n",
    "\n",
    "# Path to the root directory of this repo\n",
    "repo_root_dir = pathlib.Path().resolve().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58ea08c7-e5d3-4d07-a433-a50a85a6d78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the config file and the directory holding the test files\n",
    "CONFIG_FILE_PATH = repo_root_dir / \"streaming_2\" / \"config_files\" / \"confluent_cloud_broker.config\"\n",
    "TEST_RECO_DIR = repo_root_dir.parent / \"streaming_2\" / \"reconstructed_test_files\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716d800d-f2bb-48e1-9fb4-072617afa02a",
   "metadata": {},
   "source": [
    "## Stream Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06a8c96f-9782-45c3-954c-d472e109bfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlaceholderStreamProcessor(DataFileStreamProcessor):\n",
    "    \"\"\"Performs a placeholder task (writing out a file to the local system) for every\n",
    "    data file reconstructed from a topic\n",
    "    \"\"\"\n",
    "\n",
    "    def _process_downloaded_data_file(self, datafile, lock):\n",
    "        \"Writes out a file with a timestamp for each reconstructed file\"\n",
    "        try:\n",
    "            timestamp = datetime.datetime.now()\n",
    "            rel_filepath = datafile.relative_filepath\n",
    "            rel_fp_str = str(rel_filepath.as_posix()).replace(\"/\",\"_\").replace(\".\",\"_\")\n",
    "            output_filepath = self._output_dir / f\"{rel_fp_str}_placeholder.txt\"\n",
    "            with lock:\n",
    "                with open(output_filepath, \"w\") as filep:\n",
    "                    filep.write(\n",
    "                        f\"Processing timestamp: {timestamp.strftime('%m/%d/%Y, %H:%M:%S')}\"\n",
    "                    )\n",
    "        except Exception as exc:\n",
    "            return exc\n",
    "        return None\n",
    "    \n",
    "    @classmethod\n",
    "    def run_from_command_line(cls, args=None):\n",
    "        \"Not used in this example... stay tuned for the live coding tomorrow!\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "914c0d41-fc8e-479f-a92b-5cadc7b03eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_processor_task(stream_processor):\n",
    "    \"\"\"Run \"process_files_as_read\" for the given stream processor, and log a message\n",
    "    when it gets shuts down\n",
    "    \n",
    "    Args:\n",
    "        stream_processor (openmsistream.DataFileStreamProcessor): The stream processor to run\n",
    "    \"\"\"\n",
    "    # This call to \"process_files_as_read\" hangs until the stream processor is shut down\n",
    "    (\n",
    "        n_m_r, # The number of messages read\n",
    "        n_m_p, # The number of messages processed\n",
    "        n_f_p, # The number of files successfully processed\n",
    "        p_fps, # Paths to the most recently-processed files\n",
    "    ) = stream_processor.process_files_as_read()\n",
    "    stream_processor.close()\n",
    "    msg = f\"{n_m_r} total messages were consumed\"\n",
    "    if n_f_p > 0:\n",
    "        msg += (\n",
    "            f\", {n_m_p} messages were processed,\"\n",
    "            f\" and {n_f_p} files were successfully processed\"\n",
    "        )\n",
    "    else:\n",
    "        msg += f\" and {n_m_p} messages were successfully processed\"\n",
    "    msg += (\n",
    "        f\". Up to {stream_processor.N_RECENT_FILES} most recently \"\n",
    "        \"processed files:\\n\\t\"\n",
    "    )\n",
    "    msg += \"\\n\\t\".join([str(fp) for fp in p_fps])\n",
    "    stream_processor.logger.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f25c4ca9-98c9-407f-af62-15a2a603adbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory to store the StreamProcessor output\n",
    "STREAM_PROCESSOR_OUTPUT_DIR = repo_root_dir / \"streaming_2\" / \"processor_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "191d4b52-788b-4097-8b95-b1f95454e5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[OpenMSIConsumers 2024-07-18 15:23:31] Log files and output will be in /Users/namanparikh/Documents/GitHub/paradim/reu2024-hyperspectral-camera/streaming_2/processor_2\n",
      "[OpenMSIConsumers 2024-07-18 15:23:31] Will process files from messages in the tutorial_metadata topic using 2 threads\n",
      "[OpenMSIConsumers 2024-07-18 15:26:12] 93 total messages were consumed, 93 messages were processed, and 93 files were successfully processed. Up to 50 most recently processed files:\n",
      "\t_ipynb_checkpoints_test_data-checkpoint_txt_placeholder.txt\n",
      "\tfull_test_txt_result.npy\n",
      "\ttest_watchdog_folder__ipynb_checkpoints_test_1-checkpoint_txt_placeholder.txt\n",
      "\tfull_test_txt_result.npy\n",
      "\ttest_watchdog_process_folder__ipynb_checkpoints_test_2-checkpoint_txt_placeholder.txt\n",
      "\tfull_test_2_txt_result.npy\n",
      "\ttest_watchdog_process_folder_2__ipynb_checkpoints_test_1-checkpoint_txt_placeholder.txt\n",
      "\ttest_watchdog_txt_placeholder.txt\n",
      "\t_ipynb_checkpoints_test_data-checkpoint_txt_placeholder.txt\n",
      "\ttest_watchdog_folder__ipynb_checkpoints_test_2-checkpoint_txt_placeholder.txt\n",
      "\ttest_watchdog_process_folder__ipynb_checkpoints_test_2-checkpoint_txt_placeholder.txt\n",
      "\ttest_watchdog_process_folder__ipynb_checkpoints_test_1-checkpoint_txt_placeholder.txt\n",
      "\ttest_watchdog_folder_test_1_txt_placeholder.txt\n",
      "\ttest_watchdog_process_folder_2__ipynb_checkpoints_test_2-checkpoint_txt_placeholder.txt\n",
      "\ttest_watchdog_process_folder_test_2_txt_placeholder.txt\n",
      "\ttest_full_txt_placeholder.txt\n",
      "\ttest_watchdog_process_folder_2_test_1_txt_placeholder.txt\n",
      "\ttest_watchdog_txt_placeholder.txt\n",
      "\ttest_data_txt_placeholder.txt\n",
      "\ttest_watchdog_folder__ipynb_checkpoints_test_2-checkpoint_txt_placeholder.txt\n",
      "\ttest_watchdog_folder__ipynb_checkpoints_test_1-checkpoint_txt_placeholder.txt\n",
      "\ttest_watchdog_process_folder_2__ipynb_checkpoints_test_2-checkpoint_txt_placeholder.txt\n",
      "\ttest_watchdog_process_folder_2__ipynb_checkpoints_test_1-checkpoint_txt_placeholder.txt\n",
      "\ttest_watchdog_process_folder__ipynb_checkpoints_test_1-checkpoint_txt_placeholder.txt\n",
      "\ttest_watchdog_folder_test_2_txt_placeholder.txt\n",
      "\ttest_full_txt_placeholder.txt\n",
      "\ttest_watchdog_process_folder_test_1_txt_placeholder.txt\n",
      "\ttest_watchdog_process_folder_2_test_2_txt_placeholder.txt\n",
      "\tfull_test_txt_placeholder.txt\n",
      "\tfull_test_2_txt_placeholder.txt\n",
      "\ttest_data_txt_placeholder.npy\n",
      "\t_ipynb_checkpoints_test_data-checkpoint_txt_placeholder.npy\n",
      "\ttest_watchdog_folder_test_1_txt_placeholder.npy\n",
      "\ttest_watchdog_txt_placeholder.npy\n",
      "\ttest_watchdog_folder_test_2_txt_placeholder.npy\n",
      "\ttest_watchdog_folder__ipynb_checkpoints_test_1-checkpoint_txt_placeholder.npy\n",
      "\ttest_watchdog_process_folder_test_2_txt_placeholder.npy\n",
      "\ttest_watchdog_folder__ipynb_checkpoints_test_2-checkpoint_txt_placeholder.npy\n",
      "\ttest_watchdog_process_folder_test_1_txt_placeholder.npy\n",
      "\ttest_watchdog_process_folder__ipynb_checkpoints_test_1-checkpoint_txt_placeholder.npy\n",
      "\ttest_watchdog_process_folder_2_test_1_txt_placeholder.npy\n",
      "\ttest_watchdog_process_folder__ipynb_checkpoints_test_2-checkpoint_txt_placeholder.npy\n",
      "\ttest_watchdog_process_folder_2_test_2_txt_placeholder.npy\n",
      "\ttest_watchdog_process_folder_2__ipynb_checkpoints_test_1-checkpoint_txt_placeholder.npy\n",
      "\tfull_test_txt_placeholder.npy\n",
      "\ttest_watchdog_process_folder_2__ipynb_checkpoints_test_2-checkpoint_txt_placeholder.npy\n",
      "\tfull_test_txt_placeholder.npy\n",
      "\ttest_full_txt_placeholder.npy\n",
      "\tfull_test_2_txt_placeholder.npy\n",
      "\tfinal_test_txt_placeholder.npy\n"
     ]
    }
   ],
   "source": [
    "# Create the StreamProcessor\n",
    "psp = PlaceholderStreamProcessor(\n",
    "    CONFIG_FILE_PATH,\n",
    "    CONSUMER_TOPIC_NAME,\n",
    "    output_dir=STREAM_PROCESSOR_OUTPUT_DIR,\n",
    "    logger=logger,\n",
    ")\n",
    "# Start running its \"process_files_as_read\" function in a separate thread\n",
    "processor_thread = Thread(\n",
    "    target=stream_processor_task,\n",
    "    args=(psp,),\n",
    ")\n",
    "processor_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa8ab38e-ae48-452b-8b64-d239846ad9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually shut down the stream processor (if running from the command line this would\n",
    "# be like typing \"q\" in the Terminal window)\n",
    "psp.control_command_queue.put(\"q\")\n",
    "processor_thread.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
